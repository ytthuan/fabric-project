# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "75a9b817-00c8-4d84-8454-19eff1405b46",
# META       "default_lakehouse_name": "NYCLakehouse",
# META       "default_lakehouse_workspace_id": "33d534a2-dabe-4f41-a95b-f7819c2b0e69",
# META       "known_lakehouses": [
# META         {
# META           "id": "75a9b817-00c8-4d84-8454-19eff1405b46"
# META         }
# META       ]
# META     }
# META   }
# META }

# CELL ********************

base_path = f"Files/"


df = spark.read.format("parquet").load(f"{base_path}**/**/*.parquet")
display(df.limit(10))

df.count()


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark",
# META   "editable": true
# META }

# CELL ********************

# Code generated by Data Wrangler for PySpark DataFrame

def clean_data(df):
    # Drop duplicate rows across all columns
    df = df.dropDuplicates()
    # Drop rows with missing data across all columns
    return df

df_clean = clean_data(df)
display(df_clean)
df_clean.count()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
